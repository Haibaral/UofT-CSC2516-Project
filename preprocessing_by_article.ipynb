{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"preprocessing_by_article2.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"R8kuwFbKO37M","colab_type":"code","outputId":"36692b20-fbeb-4e5c-b2b9-79183b1e4b87","colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dBViTAKuP8ew","colab_type":"code","outputId":"749f5a72-3941-4e53-dfad-35277f2ac4c5","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["%cd 'drive/My Drive/2516Project'\n","!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/2516Project\n","data  elmo  glove  result  saved_models  saved_models_cnn\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2hZgZvX5-iPo","colab_type":"code","outputId":"470e6e4b-b746-4cde-f500-8f0c94ee3cdf","colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["import pandas as pd \n","import string\n","import numpy as np\n","import xml.etree.ElementTree as ET\n","import re\n","import sys\n","import nltk\n","from nltk.corpus import stopwords\n","from urllib.parse import urlparse\n","import argparse\n","import os\n","import time\n","from html.parser import HTMLParser\n","from nltk.corpus import stopwords as NLTK_STOPWORDS\n","from gensim.parsing.preprocessing import STOPWORDS as GENSIM_STOPWORDS\n","import tldextract\n","from collections import Counter\n","import json\n","from numbers import Number\n","\n","\n","nltk.download('stopwords')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"cf5ys8DiCptc","colab_type":"code","outputId":"b472a966-4940-47c2-8f92-67c1fa5c058b","colab":{"base_uri":"https://localhost:8080/","height":170}},"source":["!pip install tldextract"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tldextract in /usr/local/lib/python3.6/dist-packages (2.2.2)\n","Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.6/dist-packages (from tldextract) (1.4.3)\n","Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from tldextract) (2.21.0)\n","Requirement already satisfied: idna in /usr/local/lib/python3.6/dist-packages (from tldextract) (2.8)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from tldextract) (46.1.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from requests-file>=1.4->tldextract) (1.12.0)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->tldextract) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->tldextract) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.1.0->tldextract) (2020.4.5.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"XFKGPeyMSxw2","colab_type":"code","outputId":"bbe1dca4-50b5-4f8f-e435-8c1bafe32ff3","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["!pip install allennlp"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Collecting allennlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/bb/041115d8bad1447080e5d1e30097c95e4b66e36074277afce8620a61cee3/allennlp-0.9.0-py3-none-any.whl (7.6MB)\n","\u001b[K     |████████████████████████████████| 7.6MB 3.1MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.22.2.post1)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n","Collecting jsonpickle\n","  Downloading https://files.pythonhosted.org/packages/cb/e0/54421447d55bc7304a785be9ec81f28e1e8a8c6619b0e35154ed8f1b7761/jsonpickle-1.4-py2.py3-none-any.whl\n","Collecting parsimonious>=0.8.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/02/fc/067a3f89869a41009e1a7cdfb14725f8ddd246f30f63c645e8ef8a1c56f4/parsimonious-0.8.1.tar.gz (45kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.0MB/s \n","\u001b[?25hRequirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.12.39)\n","Collecting gevent>=1.3.6\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/95/b53b78b15abbe547bed7381ca9c8319c86d6b646a30d0831e26c307a5fa7/gevent-1.5.0-cp36-cp36m-manylinux2010_x86_64.whl (5.1MB)\n","\u001b[K     |████████████████████████████████| 5.1MB 40.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.18.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.4.1)\n","Collecting word2number>=1.1\n","  Downloading https://files.pythonhosted.org/packages/4a/29/a31940c848521f0725f0df6b25dca8917f13a2025b0e8fcbe5d0457e45e6/word2number-1.1.zip\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.10.0)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2018.9)\n","Collecting ftfy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d8/5e877ac5e827eaa41a7ea8c0dc1d3042e05d7e337604dc2aedb854e7b500/ftfy-5.7.tar.gz (58kB)\n","\u001b[K     |████████████████████████████████| 61kB 7.5MB/s \n","\u001b[?25hCollecting jsonnet>=0.10.0; sys_platform != \"win32\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/b8/a8588d4010f13716a324f55d23999259bad9db2320f4fe919a66b2f651f3/jsonnet-0.15.0.tar.gz (255kB)\n","\u001b[K     |████████████████████████████████| 256kB 48.0MB/s \n","\u001b[?25hCollecting pytorch-pretrained-bert>=0.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n","\u001b[K     |████████████████████████████████| 133kB 48.6MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.38.0)\n","Collecting responses>=0.7\n","  Downloading https://files.pythonhosted.org/packages/01/0c/e4da4191474e27bc41bedab2bf249b27d9261db749f59769d7e7ca8feead/responses-0.10.14-py2.py3-none-any.whl\n","Collecting numpydoc>=0.8.0\n","  Downloading https://files.pythonhosted.org/packages/b0/70/4d8c3f9f6783a57ac9cc7a076e5610c0cc4a96af543cafc9247ac307fbfe/numpydoc-0.9.2.tar.gz\n","Collecting flask-cors>=3.0.7\n","  Downloading https://files.pythonhosted.org/packages/78/38/e68b11daa5d613e3a91e4bf3da76c94ac9ee0d9cd515af9c1ab80d36f709/Flask_Cors-3.0.8-py2.py3-none-any.whl\n","Collecting unidecode\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n","\u001b[K     |████████████████████████████████| 245kB 47.7MB/s \n","\u001b[?25hCollecting conllu==1.3.1\n","  Downloading https://files.pythonhosted.org/packages/ae/54/b0ae1199f3d01666821b028cd967f7c0ac527ab162af433d3da69242cea2/conllu-1.3.1-py2.py3-none-any.whl\n","Requirement already satisfied: flask>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.1.2)\n","Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.21.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n","Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.1)\n","Requirement already satisfied: sqlparse>=0.2.4 in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.3.1)\n","Collecting pytorch-transformers==1.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/89/ad0d6bb932d0a51793eaabcf1617a36ff530dc9ab9e38f765a35dc293306/pytorch_transformers-1.1.0-py3-none-any.whl (158kB)\n","\u001b[K     |████████████████████████████████| 163kB 49.1MB/s \n","\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.5.3)\n","Collecting tensorboardX>=1.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/f1/5843425495765c8c2dd0784a851a93ef204d314fc87bcc2bbb9f662a3ad1/tensorboardX-2.0-py2.py3-none-any.whl (195kB)\n","\u001b[K     |████████████████████████████████| 204kB 50.6MB/s \n","\u001b[?25hCollecting spacy<2.2,>=2.1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/5b/e07dd3bf104237bce4b398558b104c8e500333d6f30eabe3fa9685356b7d/spacy-2.1.9-cp36-cp36m-manylinux1_x86_64.whl (30.8MB)\n","\u001b[K     |████████████████████████████████| 30.9MB 144kB/s \n","\u001b[?25hCollecting flaky\n","  Downloading https://files.pythonhosted.org/packages/fe/12/0f169abf1aa07c7edef4855cca53703d2e6b7ecbded7829588ac7e7e3424/flaky-3.6.1-py2.py3-none-any.whl\n","Collecting overrides\n","  Downloading https://files.pythonhosted.org/packages/72/dd/ac49f9c69540d7e09210415801a05d0a54d4d0ca8401503c46847dacd3a0/overrides-2.8.0.tar.gz\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->allennlp) (0.14.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (46.1.3)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.12.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.8.1)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.3.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (19.3.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (8.2.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from jsonpickle->allennlp) (1.6.0)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.9.5)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.39 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (1.15.39)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->allennlp) (0.3.3)\n","Collecting greenlet>=0.4.14; platform_python_implementation == \"CPython\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bf/45/142141aa47e01a5779f0fa5a53b81f8379ce8f2b1cd13df7d2f1d751ae42/greenlet-0.4.15-cp36-cp36m-manylinux1_x86_64.whl (41kB)\n","\u001b[K     |████████████████████████████████| 51kB 6.3MB/s \n","\u001b[?25hRequirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from ftfy->allennlp) (0.1.9)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert>=0.6.0->allennlp) (2019.12.20)\n","Requirement already satisfied: sphinx>=1.6.5 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (1.8.5)\n","Requirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc>=0.8.0->allennlp) (2.11.2)\n","Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.1.0)\n","Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (1.0.1)\n","Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask>=1.0.2->allennlp) (7.1.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2020.4.5.1)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.8)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.8.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (1.2.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (0.10.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=2.2.3->allennlp) (2.4.7)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/f4/2d5214cbf13d06e7cb2c20d84115ca25b53ea76fa1f0ade0e3c9749de214/sentencepiece-0.1.85-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n","\u001b[K     |████████████████████████████████| 1.0MB 32.9MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX>=1.2->allennlp) (3.10.0)\n","Collecting thinc<7.1.0,>=7.0.8\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/a5/9ace20422e7bb1bdcad31832ea85c52a09900cd4a7ce711246bfb92206ba/thinc-7.0.8-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n","\u001b[K     |████████████████████████████████| 2.1MB 35.1MB/s \n","\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n","Collecting plac<1.0.0,>=0.9.6\n","  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (2.0.3)\n","Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (1.0.2)\n","Collecting preshed<2.1.0,>=2.0.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/20/93/f222fb957764a283203525ef20e62008675fd0a14ffff8cc1b1490147c63/preshed-2.0.1-cp36-cp36m-manylinux1_x86_64.whl (83kB)\n","\u001b[K     |████████████████████████████████| 92kB 10.3MB/s \n","\u001b[?25hRequirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.2,>=2.1.0->allennlp) (0.6.0)\n","Collecting blis<0.3.0,>=0.2.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl (3.2MB)\n","\u001b[K     |████████████████████████████████| 3.2MB 39.0MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->jsonpickle->allennlp) (3.1.0)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.39->boto3->allennlp) (0.15.2)\n","Requirement already satisfied: imagesize in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.0)\n","Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.0.0)\n","Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (0.7.12)\n","Requirement already satisfied: sphinxcontrib-websupport in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (1.2.1)\n","Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.1.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (20.3)\n","Requirement already satisfied: babel!=2.0,>=1.3 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.6.5->numpydoc>=0.8.0->allennlp) (2.8.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc>=0.8.0->allennlp) (1.1.1)\n","Building wheels for collected packages: parsimonious, word2number, ftfy, jsonnet, numpydoc, overrides\n","  Building wheel for parsimonious (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for parsimonious: filename=parsimonious-0.8.1-cp36-none-any.whl size=42712 sha256=947ab432ca03ba03737d906f5e7c86726d65471eaac61178a8f8fb375a6aba60\n","  Stored in directory: /root/.cache/pip/wheels/b7/8d/e7/a0e74217da5caeb3c1c7689639b6d28ddbf9985b840bc96a9a\n","  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for word2number: filename=word2number-1.1-cp36-none-any.whl size=5587 sha256=67f8d1c37ef6f50b74eaaab096ab2b1d917b2ef585f979334f301ab0cd240f77\n","  Stored in directory: /root/.cache/pip/wheels/46/2f/53/5f5c1d275492f2fce1cdab9a9bb12d49286dead829a4078e0e\n","  Building wheel for ftfy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for ftfy: filename=ftfy-5.7-cp36-none-any.whl size=44593 sha256=db492777ef91c01791c7bbfdb3ed804e312d985cad29eb87a337d7e91d98d066\n","  Stored in directory: /root/.cache/pip/wheels/8e/da/59/6c8925d571aacade638a0f515960c21c0887af1bfe31908fbf\n","  Building wheel for jsonnet (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jsonnet: filename=jsonnet-0.15.0-cp36-cp36m-linux_x86_64.whl size=3319805 sha256=f4cc79729d5c5af4a58194d68d4e51088d59bd65c9238b3d4c163d6d9437d6b0\n","  Stored in directory: /root/.cache/pip/wheels/57/63/2e/da89cfe1ba08550bd7262d5d9c027edc313980c3b85b3b0a38\n","  Building wheel for numpydoc (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for numpydoc: filename=numpydoc-0.9.2-cp36-none-any.whl size=31893 sha256=b467b995e7bb8deeaec0476b49edf2a520d7a55cfc9b2c07c1990246cb780e59\n","  Stored in directory: /root/.cache/pip/wheels/96/f3/52/25c8e1f40637661d27feebc61dae16b84c7cdd93b8bc3d7486\n","  Building wheel for overrides (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for overrides: filename=overrides-2.8.0-cp36-none-any.whl size=5609 sha256=c40f01f1745721a5a88e2c0d35f3a60e6fdc7f9909baf9081df094b2c66f2873\n","  Stored in directory: /root/.cache/pip/wheels/df/f1/ba/eaf6cd7d284d2f257dc71436ce72d25fd3be5a5813a37794ab\n","Successfully built parsimonious word2number ftfy jsonnet numpydoc overrides\n","\u001b[31mERROR: en-core-web-sm 2.2.5 has requirement spacy>=2.2.2, but you'll have spacy 2.1.9 which is incompatible.\u001b[0m\n","Installing collected packages: jsonpickle, parsimonious, greenlet, gevent, word2number, ftfy, jsonnet, pytorch-pretrained-bert, responses, numpydoc, flask-cors, unidecode, conllu, sentencepiece, pytorch-transformers, tensorboardX, plac, preshed, blis, thinc, spacy, flaky, overrides, allennlp\n","  Found existing installation: plac 1.1.3\n","    Uninstalling plac-1.1.3:\n","      Successfully uninstalled plac-1.1.3\n","  Found existing installation: preshed 3.0.2\n","    Uninstalling preshed-3.0.2:\n","      Successfully uninstalled preshed-3.0.2\n","  Found existing installation: blis 0.4.1\n","    Uninstalling blis-0.4.1:\n","      Successfully uninstalled blis-0.4.1\n","  Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","  Found existing installation: spacy 2.2.4\n","    Uninstalling spacy-2.2.4:\n","      Successfully uninstalled spacy-2.2.4\n","Successfully installed allennlp-0.9.0 blis-0.2.4 conllu-1.3.1 flaky-3.6.1 flask-cors-3.0.8 ftfy-5.7 gevent-1.5.0 greenlet-0.4.15 jsonnet-0.15.0 jsonpickle-1.4 numpydoc-0.9.2 overrides-2.8.0 parsimonious-0.8.1 plac-0.9.6 preshed-2.0.1 pytorch-pretrained-bert-0.6.2 pytorch-transformers-1.1.0 responses-0.10.14 sentencepiece-0.1.85 spacy-2.1.9 tensorboardX-2.0 thinc-7.0.8 unidecode-1.1.1 word2number-1.1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"S01N3Q04VKS2","colab_type":"text"},"source":["# Parsing HTML"]},{"cell_type":"code","metadata":{"id":"jPWxoqGxDQxu","colab_type":"code","colab":{}},"source":["def clean_text(text):\n","    text = text.replace(\"”\", \"\\\"\")\n","    text = text.replace(\"“\", \"\\\"\")\n","    text = text.replace(\"’\", \"'\")\n","    text = text.replace(\"&amp;\", \"&\")\n","    text = text.replace(\" _\", \" \")\n","    text = text.replace(\"–\", \"-\")\n","    text = text.replace(\"&lt;\", \"<\")\n","    text = text.replace(\"&gt;\", \">\")\n","    text = text.replace(\"<p>\", \" \")\n","    text = text.replace(\"</p>\", \" \")\n","    text_to_space = re.compile('&#160;')\n","    text = text_to_space.subn(' ', text)[0]\n","    return text\n","\n","class MyHTMLParser(HTMLParser):\n","    def __init__(self):\n","        HTMLParser.__init__(self)\n","        self.ignore = False\n","        self.data = []\n","        self.temp = []\n","        self.type1 = ['script', 'style']\n","        self.type2 = ['p', 'br']\n","\n","    def append_paragraph(self):\n","        if len(self.temp) > 0:\n","            self.data.append(self.temp)\n","            self.temp = []\n","\n","    def handle_starttag(self, tag, attrs):\n","        if tag in self.type1:\n","            self.ignore = True\n","        elif tag in self.type2:\n","            self.append_paragraph()\n","\n","    def handle_endtag(self, tag):\n","        if tag in self.type1:\n","            self.ignore = False\n","        elif tag in self.type2:\n","            self.append_paragraph()\n","\n","    def handle_startendtag(self, tag, attrs):\n","        if tag in self.type2:\n","            self.append_paragraph()\n","\n","    def handle_data(self, data):\n","        if not self.ignore:\n","            self.temp.append(data)\n","\n","    def close(self):\n","        HTMLParser.close(self)\n","        self.append_paragraph()\n","\n","    def reset(self):\n","        HTMLParser.reset(self)\n","        self.data = []\n","        self.temp = []\n","\n","    def clean_para(self, text):\n","        text = clean_text(text)\n","        text = text.replace('\\n', ' ').replace('\\r', ' ').replace('\\t', ' ')\n","        text = ' '.join(text.split()).strip()\n","        return text\n","\n","    def paragraphs(self):\n","        pars = []\n","        for par in self.data:\n","            if len(par) > 0:\n","                text = self.clean_para(''.join(par)).strip()\n","                if text:\n","                    pars.append(text)\n","        return pars\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NYkJ1X-xGJ55","colab_type":"text"},"source":["# Preprocessing "]},{"cell_type":"code","metadata":{"id":"-WTs2gkKXIAt","colab_type":"code","colab":{}},"source":["stopwords = set(NLTK_STOPWORDS.words('english'))\n","for w in GENSIM_STOPWORDS:\n","    stopwords.add(w)\n","stopwords.add(\"'s\")\n","stopword_list = list(stopwords)\n","for s in stopword_list:\n","    stopwords.add(s.capitalize())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ljqILvXhGImd","colab_type":"code","colab":{}},"source":["class Article_to_Line:\n","    def __init__(self, stream, features):\n","        self.stream = stream\n","        self.features = features \n","\n","    def extract_features(self, doc, features):\n","        result = []\n","        for f in features:\n","            val = doc.get(f)\n","            result.append(val)\n","        return result\n","\n","    def __call__(self, article):\n","        vals = self.extract_features(article, self.features)\n","        strings = []\n","        for i in range(len(vals)):\n","            val = vals[i]\n","            if isinstance(val, str):\n","                strings.append(val)\n","            elif isinstance(val, Number):\n","                strings.append(str(val))\n","            elif isinstance(val, list):\n","                strings.append(json.dumps(val))\n","            elif isinstance(val, dict):\n","                strings.append(json.dumps(val))\n","        print(article['id'], article.get('target'),\n","                article.get('bias'), article.get('domain'),\n","                \"\\t\".join(strings), file=self.stream, sep=\"\\t\")\n","\n","\n","class AddTarget:\n","    def __init__(self, target_, bias_, url_):\n","        self.target_ = target_\n","        self.bias_ = bias_\n","        self.url_ = url_\n","    def __call__(self, article):\n","        id = article['id']\n","        target = self.target_[id]\n","        bias = self.bias_[id]\n","        url = self.url_[id]\n","        article['target'] = target\n","        article['bias'] = bias\n","        article['url'] = url\n","\n","\n","class AddTitle:\n","    def __call__(self, article):\n","        article['title'] = clean_text(article['et'].attrib[\"title\"])\n","\n","\n","class AddText:\n","    def __init__(self):\n","        self.parser = None \n","    \n","    def __call__(self, article):\n","        if self.parser is None:\n","            self.parser = MyHTMLParser()\n","        self.parser.reset()\n","        self.parser.feed(article['xml'])\n","        self.parser.close()\n","        pars = self.parser.paragraphs()\n","        article['pars'] = pars\n","        text = \" \".join(pars)\n","        article['text'] = text\n","\n","\n","class RemoveParagraphs:\n","    def __call__(self, article):\n","        del article['pars']\n","\n","\n","class Filtered_Text:\n","    def f_tokens(self, tokens):\n","        punctuation = '.,:;?!\"\\'\\'``+={}[]()#~$--'\n","        re_num = re.compile('^-?[0-9.,]+([eE^][0-9]+)?(th)?$')\n","        filtered = [token for token in tokens if token not in punctuation and \"_\" not in token and token not in stopwords]\n","        filtered = [re_num.subn(\"<num>\", token)[0] for token in filtered]\n","        return filtered\n","    \n","    def __call__(self, article):\n","        text, title = article['text_tokens'], article['title_tokens']\n","        tokens = self.f_tokens([t[0] for t in title])\n","        tokens.append(\"<sep_t2d>\")\n","        if article.get('link_domains_all'):\n","            tokens.extend([\"DOMAIN_\" + d for d in article['link_domains']])\n","        tokens.append(\"<sep_d2a>\")\n","        tokens.extend(self.f_tokens([t[0] for sent in text for t in sent]))\n","        token_string = \" \".join(tokens)\n","        article['text_all_filtered'] = token_string\n","\n","\n","class NlpSpacy:\n","    def __init__(self):\n","        self.initialized = False\n","\n","    def __call__(self, article):\n","        if not self.initialized:\n","            self.nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\"])\n","            self.nlp.add_pipe(self.nlp.create_pipe('sentencizer'))\n","            self.initialized = True\n","        pars = article['pars']\n","        article['n_p'] = len(pars)\n","        n_p_filled = 0\n","        docs = list(self.nlp.pipe(pars))\n","        allthree = [[[t.text, t.pos_, t.lemma_] for t in s] for doc in docs for s in doc.sents]\n","        article['n_p_filled'] = n_p_filled\n","        article['text_tokens'] = allthree\n","        ents = [ent.text for doc in docs for ent in doc.ents if ent.text[0].isupper()]\n","        article['text_ents'] = ents\n","        title = article['title']\n","        doc = self.nlp(title)\n","        allthree = [(t.text, t.pos_, t.lemma_) for s in list(doc.sents) for t in s]\n","        article['title_tokens'] = allthree\n","        ents = [ent.text for ent in doc.ents if ent.text[0].isupper()]\n","        article['title_ents'] = ents\n","\n","\n","class SequenceSentences:\n","    def __call__(self, article):\n","        title_sent = \" \".join([t[0] for t in article['title_tokens']])\n","        article['title_sent'] = title_sent\n","        \n","        domain_sent = \"\"\n","        if article.get('link_domains_all'):\n","            domain_sent = \" \".join([\"DOMAIN_\" + d for d in article['link_domains']])\n","        article['domain_sent'] = domain_sent\n","\n","        all_article = []\n","        first = True\n","        for sent in article['text_tokens']:\n","            if first:\n","                first = False\n","            else:\n","                all_article.append(\"<splt>\")\n","            for t in sent:\n","                all_article.append(t[0])\n","\n","        article_sent = \" \".join(all_article)\n","        article['article_sent'] = article_sent\n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GO7z_q3p9h2z","colab_type":"text"},"source":["# Convert to TSV"]},{"cell_type":"code","metadata":{"id":"aT18rPhRGxVT","colab_type":"code","outputId":"a3458439-493c-4297-ef1f-83dcb05df1a8","colab":{"base_uri":"https://localhost:8080/","height":377}},"source":["!pip install -U spacy"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied, skipping upgrade: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n","Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.2)\n","Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n","Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n","Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.38.0)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (46.1.3)\n","Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n","Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.4.5.1)\n","Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n","Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.6.0)\n","Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"g6gvILcSmhqv","colab_type":"code","colab":{}},"source":["import spacy"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"eDHC78p5EkBw","colab_type":"code","outputId":"b9271385-d22e-43ce-a25d-70bed72e33c9","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["def process_xml(file, funcs):\n","    tree = ET.iterparse(file)\n","    n = 0\n","    for event, element in tree:\n","        if element.tag == \"article\":\n","            attrs = element.attrib\n","            id = attrs['id']\n","            published = attrs.get(\"published-at\")\n","            xml = ET.tostring(element, encoding=\"utf-8\", method=\"xml\").decode()\n","            article = {\n","                'id': id,\n","                'xml': xml,\n","                'published-at': published,\n","                'et': element\n","            }\n","            for func in funcs:\n","                func(article)\n","            del article['et']\n","            n += 1\n","\n","    return n\n","\n","article_path = 'data/articles-training-byarticle-20181122.xml'\n","target_path = 'data/ground-truth-training-byarticle-20181122.xml'\n","out_path = 'data/train.text.tsv'\n","features = ['article_sent', 'title_sent']\n","\n","print(\"loading...\")\n","\n","target, bias, url = {}, {}, {}\n","tree = ET.parse(target_path)\n","root = tree.getroot()\n","for child in root:\n","    attribs = child.attrib\n","    id = attribs[\"id\"]\n","    target[id], bias[id], url[id] = attribs[\"hyperpartisan\"], attribs.get(\"bias\"), attribs[\"url\"]\n","\n","pipeline = [AddTarget(target, bias, url)]\n","\n","pipeline.extend([\n","    AddTitle(),\n","    AddText(),\n","    NlpSpacy(),\n","    Filtered_Text(),\n","    SequenceSentences(),\n","    RemoveParagraphs(),\n","])\n","\n","with open(out_path, \"wt\", encoding=\"utf8\") as out:\n","    pipeline.append(Article_to_Line(out, features))\n","    n_processed = process_xml(article_path, pipeline)\n","    print(\"Processed articles:\", n_processed)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["loading...\n","Processed articles: 645\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d-JCWl6VCHUq","colab_type":"code","outputId":"487691b3-c736-45c8-fae9-701c70918f1b","colab":{"base_uri":"https://localhost:8080/","height":419}},"source":["pd.read_csv('data/train.text.tsv', sep='\\t', header=None)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>True</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>From flickr.com : Money { MID-161793 } &lt;splt&gt; ...</td>\n","      <td>Kucinich : Reclaiming the money power</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>True</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>Donald Trump ran on many braggadocios and larg...</td>\n","      <td>Trump Just Woke Up &amp; Viciously Attacked Puerto...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>True</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>Photo By Justin Sullivan / Getty Images &lt;splt&gt;...</td>\n","      <td>Liberals wailing about gun control , but what ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>True</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>After Colin Kaepernick rightly chose to kneel ...</td>\n","      <td>Laremy Tunsil joins NFL players in kneeling du...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>Almost a half - century ago , in 1968 , the Un...</td>\n","      <td>It 's 1968 All Over Again</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>640</th>\n","      <td>640</td>\n","      <td>True</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>Donald Trump . &lt;splt&gt; Photo from whitehouse.go...</td>\n","      <td>Trump Turns his Back on American Workers</td>\n","    </tr>\n","    <tr>\n","      <th>641</th>\n","      <td>641</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>President Donald Trump on Tuesday began disman...</td>\n","      <td>Cummins : Rescinding DACA ‘ discriminatory , h...</td>\n","    </tr>\n","    <tr>\n","      <th>642</th>\n","      <td>642</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>The US Supreme Court has ruled that Donald Tru...</td>\n","      <td>Trump travel ban can be enforced , says US Sup...</td>\n","    </tr>\n","    <tr>\n","      <th>643</th>\n","      <td>643</td>\n","      <td>False</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>Ex - FBI Director James Comey went rogue , acc...</td>\n","      <td>VIDEO- AG SESSIONS : Comey Went Rogue In Hilla...</td>\n","    </tr>\n","    <tr>\n","      <th>644</th>\n","      <td>644</td>\n","      <td>True</td>\n","      <td>None</td>\n","      <td>None</td>\n","      <td>Ashley Judd is the absolute worst . &lt;splt&gt; I w...</td>\n","      <td>Hollywood Actors Who Condemn Trump but Were Si...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>645 rows × 6 columns</p>\n","</div>"],"text/plain":["       0  ...                                                  5\n","0      0  ...              Kucinich : Reclaiming the money power\n","1      1  ...  Trump Just Woke Up & Viciously Attacked Puerto...\n","2      2  ...  Liberals wailing about gun control , but what ...\n","3      3  ...  Laremy Tunsil joins NFL players in kneeling du...\n","4      4  ...                          It 's 1968 All Over Again\n","..   ...  ...                                                ...\n","640  640  ...           Trump Turns his Back on American Workers\n","641  641  ...  Cummins : Rescinding DACA ‘ discriminatory , h...\n","642  642  ...  Trump travel ban can be enforced , says US Sup...\n","643  643  ...  VIDEO- AG SESSIONS : Comey Went Rogue In Hilla...\n","644  644  ...  Hollywood Actors Who Condemn Trump but Were Si...\n","\n","[645 rows x 6 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"KViv7tzUbZAF","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}